{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A BERT model with history answer embedding (HAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "from cqa_supports import *\n",
    "from cqa_flags import FLAGS\n",
    "from cqa_model import *\n",
    "from cqa_gen_batches import *\n",
    "\n",
    "from scorer import external_call # quac official evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps : 20\n",
      "dataset : quac\n",
      "predict_batch_size : 6\n",
      "num_tpu_cores : 8\n",
      "f : \n",
      "doc_stride : 128\n",
      "save_checkpoints_steps : 1000\n",
      "train_batch_size : 6\n",
      "warmup_proportion : 0.1\n",
      "max_answer_length : 30\n",
      "iterations_per_loop : 1000\n",
      "output_dir : /mnt/scratch/chenqu/bert_out/xxx/\n",
      "verbose_logging : False\n",
      "evaluation_steps : 5\n",
      "evaluate_after : 0\n",
      "num_train_epochs : 2.0\n",
      "do_predict : True\n",
      "tpu_name : None\n",
      "gcp_project : None\n",
      "learning_rate : 3e-05\n",
      "max_query_length : 64\n",
      "use_tpu : False\n",
      "quac_train_file : /mnt/scratch/chenqu/quac_original/train_v0.2.json\n",
      "load_small_portion : True\n",
      "cache_dir : /mnt/scratch/chenqu/bert_out/cache/\n",
      "bert_config_file : /mnt/scratch/chenqu/bert/uncased_L-12_H-768_A-12/bert_config.json\n",
      "max_considered_history_turns : 11\n",
      "history : 6\n",
      "n_best_size : 4\n",
      "master : None\n",
      "vocab_file : /mnt/scratch/chenqu/bert/uncased_L-12_H-768_A-12/vocab.txt\n",
      "do_lower_case : True\n",
      "max_seq_length : 384\n",
      "init_checkpoint : /mnt/scratch/chenqu/bert/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "do_train : True\n",
      "quac_predict_file : /mnt/scratch/chenqu/quac_original/val_v0.2.json\n",
      "tpu_zone : None\n",
      "WARNING:tensorflow:<<<<<<<<<< load_small_portion is on! >>>>>>>>>>\n",
      "attempting to load train features from cache\n",
      "WARNING:tensorflow:<<<<<<<<<< load_small_portion is on! >>>>>>>>>>\n",
      "attempting to load val features from cache\n",
      "***** Running training *****\n",
      "  Num orig examples = %d 78\n",
      "  Num train_features = %d 477\n",
      "  Batch size = %d 6\n",
      "  Num steps = %d 20\n",
      "training step: 0, total_loss: 5.972436904907227\n",
      "training step: 1, total_loss: 4.52077579498291\n",
      "training step: 2, total_loss: 4.674416542053223\n",
      "training step: 3, total_loss: 3.602847099304199\n",
      "training step: 4, total_loss: 4.1603684425354\n",
      "training step: 5, total_loss: 3.5615620613098145\n",
      "epoch finished!\n",
      "INFO:tensorflow:Writing predictions to: /mnt/scratch/chenqu/bert_out/xxx/predictions_5.json\n",
      "INFO:tensorflow:Writing nbest to: /mnt/scratch/chenqu/bert_out/xxx/nbest_predictions_5.json\n",
      "evaluation: 5, total_loss: 3.707427978515625, f1: 0.19278305349995645, followup: 0.0, yesno: 0.27233115468409586, heq: 0.16339869281045752, dheq: 0.0\n",
      "\n",
      "Model saved in path /mnt/scratch/chenqu/bert_out/xxx//model_5.ckpt\n",
      "training step: 6, total_loss: 4.4718780517578125\n",
      "training step: 7, total_loss: 4.34637451171875\n",
      "training step: 8, total_loss: 4.460916519165039\n",
      "training step: 9, total_loss: 5.551098346710205\n",
      "training step: 10, total_loss: 3.9790167808532715\n",
      "epoch finished!\n",
      "INFO:tensorflow:Writing predictions to: /mnt/scratch/chenqu/bert_out/xxx/predictions_10.json\n",
      "INFO:tensorflow:Writing nbest to: /mnt/scratch/chenqu/bert_out/xxx/nbest_predictions_10.json\n",
      "evaluation: 10, total_loss: 3.4878361225128174, f1: 0.23148148148148148, followup: 0.0, yesno: 0.27233115468409586, heq: 0.23148148148148148, dheq: 0.0\n",
      "\n",
      "Model saved in path /mnt/scratch/chenqu/bert_out/xxx//model_10.ckpt\n",
      "training step: 11, total_loss: 5.126951217651367\n",
      "training step: 12, total_loss: 5.825507640838623\n",
      "training step: 13, total_loss: 4.534276008605957\n",
      "training step: 14, total_loss: 5.225592136383057\n",
      "training step: 15, total_loss: 5.108677864074707\n",
      "epoch finished!\n",
      "INFO:tensorflow:Writing predictions to: /mnt/scratch/chenqu/bert_out/xxx/predictions_15.json\n",
      "INFO:tensorflow:Writing nbest to: /mnt/scratch/chenqu/bert_out/xxx/nbest_predictions_15.json\n",
      "evaluation: 15, total_loss: 3.3115992546081543, f1: 0.23148148148148148, followup: 0.0, yesno: 0.27233115468409586, heq: 0.23148148148148148, dheq: 0.0\n",
      "\n",
      "Model saved in path /mnt/scratch/chenqu/bert_out/xxx//model_15.ckpt\n",
      "training step: 16, total_loss: 4.976173400878906\n",
      "training step: 17, total_loss: 6.099612236022949\n",
      "training step: 18, total_loss: 5.1085710525512695\n",
      "training step: 19, total_loss: 3.191128730773926\n",
      "training step: 20, total_loss: 3.6223578453063965\n",
      "epoch finished!\n",
      "INFO:tensorflow:Writing predictions to: /mnt/scratch/chenqu/bert_out/xxx/predictions_20.json\n",
      "INFO:tensorflow:Writing nbest to: /mnt/scratch/chenqu/bert_out/xxx/nbest_predictions_20.json\n",
      "evaluation: 20, total_loss: 3.2565088272094727, f1: 0.220029929499083, followup: 0.0, yesno: 0.27233115468409586, heq: 0.2178649237472767, dheq: 0.0\n",
      "\n",
      "Model saved in path /mnt/scratch/chenqu/bert_out/xxx//model_20.ckpt\n"
     ]
    }
   ],
   "source": [
    "for key in FLAGS:\n",
    "    print(key, ':', FLAGS[key].value)\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "tf.gfile.MakeDirs(FLAGS.output_dir)\n",
    "tf.gfile.MakeDirs(FLAGS.output_dir + '/summaries/train/')\n",
    "tf.gfile.MakeDirs(FLAGS.output_dir + '/summaries/val/')\n",
    "tf.gfile.MakeDirs(FLAGS.output_dir + '/summaries/rl/')\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "if FLAGS.do_train:\n",
    "    # read in training data, generate training features, and generate training batches\n",
    "    train_examples = None\n",
    "    num_train_steps = None\n",
    "    num_warmup_steps = None\n",
    "    train_file = FLAGS.quac_train_file\n",
    "    train_examples = read_quac_examples(input_file=train_file, is_training=True)\n",
    "        \n",
    "    \n",
    "    # we attempt to read features from cache\n",
    "    features_fname = FLAGS.cache_dir + FLAGS.dataset.lower() + \\\n",
    "               '/train_features_{}_{}.pkl'.format(FLAGS.load_small_portion, FLAGS.max_considered_history_turns)\n",
    "    example_tracker_fname = FLAGS.cache_dir + FLAGS.dataset.lower() + \\\n",
    "               '/example_tracker_{}_{}.pkl'.format(FLAGS.load_small_portion, FLAGS.max_considered_history_turns)\n",
    "    variation_tracker_fname = FLAGS.cache_dir + FLAGS.dataset.lower() + \\\n",
    "               '/variation_tracker_{}_{}.pkl'.format(FLAGS.load_small_portion, FLAGS.max_considered_history_turns)\n",
    "    example_features_nums_fname = FLAGS.cache_dir + FLAGS.dataset.lower() + \\\n",
    "               '/example_features_nums_{}_{}.pkl'.format(FLAGS.load_small_portion, FLAGS.max_considered_history_turns)\n",
    "        \n",
    "    try:\n",
    "        print('attempting to load train features from cache')\n",
    "        with open(features_fname, 'rb') as handle:\n",
    "            train_features = pickle.load(handle)\n",
    "        with open(example_tracker_fname, 'rb') as handle:\n",
    "            example_tracker = pickle.load(handle)\n",
    "        with open(variation_tracker_fname, 'rb') as handle:\n",
    "            variation_tracker = pickle.load(handle)\n",
    "        with open(example_features_nums_fname, 'rb') as handle:\n",
    "            example_features_nums = pickle.load(handle)\n",
    "    except:\n",
    "        print('train feature cache does not exist, generating')\n",
    "        train_features, example_tracker, variation_tracker, \\\n",
    "                                example_features_nums = convert_examples_to_variations_and_then_features(\n",
    "                                        examples=train_examples, tokenizer=tokenizer, \n",
    "                                        max_seq_length=FLAGS.max_seq_length, doc_stride=FLAGS.doc_stride, \n",
    "                                        max_query_length=FLAGS.max_query_length, \n",
    "                                        max_considered_history_turns=FLAGS.max_considered_history_turns, \n",
    "                                        is_training=True)\n",
    "        with open(features_fname, 'wb') as handle:\n",
    "            pickle.dump(train_features, handle)\n",
    "        with open(example_tracker_fname, 'wb') as handle:\n",
    "            pickle.dump(example_tracker, handle)\n",
    "        with open(variation_tracker_fname, 'wb') as handle:\n",
    "            pickle.dump(variation_tracker, handle)     \n",
    "        with open(example_features_nums_fname, 'wb') as handle:\n",
    "            pickle.dump(example_features_nums, handle) \n",
    "        print('train features generated')\n",
    "                \n",
    "    train_batches = cqa_gen_example_aware_batches(train_features, example_tracker, variation_tracker, \n",
    "                                                  example_features_nums, FLAGS.train_batch_size, \n",
    "                                                  FLAGS.num_train_epochs, shuffle=False)\n",
    "    \n",
    "    num_train_steps = FLAGS.train_steps\n",
    "    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "\n",
    "if FLAGS.do_predict:\n",
    "    # read in validation data, generate val features\n",
    "    val_file = FLAGS.quac_predict_file\n",
    "    val_examples = read_quac_examples(input_file=val_file, is_training=False)\n",
    "    \n",
    "    # we read in the val file in json for the external_call function in the validation step\n",
    "    val_file_json = json.load(open(val_file, 'r'))['data']\n",
    "    \n",
    "    # we attempt to read features from cache\n",
    "    features_fname = FLAGS.cache_dir + FLAGS.dataset.lower() + \\\n",
    "                     '/val_features_{}_{}.pkl'.format(FLAGS.load_small_portion, FLAGS.max_considered_history_turns)\n",
    "    example_tracker_fname = FLAGS.cache_dir + FLAGS.dataset.lower() + \\\n",
    "                     '/val_example_tracker_{}_{}.pkl'.format(FLAGS.load_small_portion, FLAGS.max_considered_history_turns)\n",
    "    variation_tracker_fname = FLAGS.cache_dir + FLAGS.dataset.lower() + \\\n",
    "                     '/val_variation_tracker_{}_{}.pkl'.format(FLAGS.load_small_portion, FLAGS.max_considered_history_turns)\n",
    "    example_features_nums_fname = FLAGS.cache_dir + FLAGS.dataset.lower() + \\\n",
    "                     '/val_example_features_nums_{}_{}.pkl'.format(FLAGS.load_small_portion, FLAGS.max_considered_history_turns)\n",
    "        \n",
    "    try:\n",
    "        print('attempting to load val features from cache')\n",
    "        with open(features_fname, 'rb') as handle:\n",
    "            val_features = pickle.load(handle)\n",
    "        with open(example_tracker_fname, 'rb') as handle:\n",
    "            val_example_tracker = pickle.load(handle)\n",
    "        with open(variation_tracker_fname, 'rb') as handle:\n",
    "            val_variation_tracker = pickle.load(handle)\n",
    "        with open(example_features_nums_fname, 'rb') as handle:\n",
    "            val_example_features_nums = pickle.load(handle)\n",
    "    except:\n",
    "        print('val feature cache does not exist, generating')\n",
    "        val_features, val_example_tracker, val_variation_tracker, val_example_features_nums = \\\n",
    "                                                   convert_examples_to_variations_and_then_features(\n",
    "                                                   examples=val_examples, tokenizer=tokenizer, \n",
    "                                                   max_seq_length=FLAGS.max_seq_length, doc_stride=FLAGS.doc_stride, \n",
    "                                                   max_query_length=FLAGS.max_query_length, \n",
    "                                                   max_considered_history_turns=FLAGS.max_considered_history_turns, \n",
    "                                                   is_training=False)\n",
    "        with open(features_fname, 'wb') as handle:\n",
    "            pickle.dump(val_features, handle)\n",
    "        with open(example_tracker_fname, 'wb') as handle:\n",
    "            pickle.dump(val_example_tracker, handle)\n",
    "        with open(variation_tracker_fname, 'wb') as handle:\n",
    "            pickle.dump(val_variation_tracker, handle)  \n",
    "        with open(example_features_nums_fname, 'wb') as handle:\n",
    "            pickle.dump(val_example_features_nums, handle)\n",
    "        print('val features generated')\n",
    "    \n",
    "    \n",
    "    num_val_examples = len(val_examples)\n",
    "    \n",
    "\n",
    "# tf Graph input\n",
    "unique_ids = tf.placeholder(tf.int32, shape=[None], name='unique_ids')\n",
    "input_ids = tf.placeholder(tf.int32, shape=[None, FLAGS.max_seq_length], name='input_ids')\n",
    "input_mask = tf.placeholder(tf.int32, shape=[None, FLAGS.max_seq_length], name='input_mask')\n",
    "segment_ids = tf.placeholder(tf.int32, shape=[None, FLAGS.max_seq_length], name='segment_ids')\n",
    "start_positions = tf.placeholder(tf.int32, shape=[None], name='start_positions')\n",
    "end_positions = tf.placeholder(tf.int32, shape=[None], name='end_positions')\n",
    "history_answer_marker = tf.placeholder(tf.int32, shape=[None, FLAGS.max_seq_length], name='history_answer_marker')\n",
    "training = tf.placeholder(tf.bool, name='training')\n",
    "get_segment_rep = tf.placeholder(tf.bool, name='get_segment_rep')\n",
    "\n",
    "\n",
    "bert_representation = bert_rep(\n",
    "    bert_config=bert_config,\n",
    "    is_training=training,\n",
    "    input_ids=input_ids,\n",
    "    input_mask=input_mask,\n",
    "    segment_ids=segment_ids,\n",
    "    history_answer_marker=history_answer_marker,\n",
    "    use_one_hot_embeddings=False\n",
    "    )\n",
    "    \n",
    "(start_logits, end_logits) = cqa_model(bert_representation)\n",
    "\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "initialized_variable_names = {}\n",
    "if FLAGS.init_checkpoint:\n",
    "    (assignment_map, initialized_variable_names) = modeling.get_assigment_map_from_checkpoint(tvars, \n",
    "                                                                                              FLAGS.init_checkpoint)\n",
    "    tf.train.init_from_checkpoint(FLAGS.init_checkpoint, assignment_map)\n",
    "\n",
    "# compute loss\n",
    "seq_length = modeling.get_shape_list(input_ids)[1]\n",
    "def compute_loss(logits, positions):\n",
    "    one_hot_positions = tf.one_hot(\n",
    "        positions, depth=seq_length, dtype=tf.float32)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "    loss = -tf.reduce_mean(tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
    "    return loss\n",
    "\n",
    "# get the max prob for the predicted start/end position\n",
    "start_probs = tf.nn.softmax(start_logits, axis=-1)\n",
    "start_prob = tf.reduce_max(start_probs, axis=-1)\n",
    "end_probs = tf.nn.softmax(end_logits, axis=-1)\n",
    "end_prob = tf.reduce_max(end_probs, axis=-1)\n",
    "\n",
    "start_loss = compute_loss(start_logits, start_positions)\n",
    "end_loss = compute_loss(end_logits, end_positions)\n",
    "total_loss = (start_loss + end_loss) / 2.0\n",
    "tf.summary.scalar('total_loss', total_loss)\n",
    "\n",
    "\n",
    "if FLAGS.do_train:\n",
    "    train_op = optimization.create_optimizer(total_loss, FLAGS.learning_rate, num_train_steps, num_warmup_steps, False)\n",
    "\n",
    "    print(\"***** Running training *****\")\n",
    "    print(\"  Num orig examples = %d\", len(train_examples))\n",
    "    print(\"  Num train_features = %d\", len(train_features))\n",
    "    print(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
    "    print(\"  Num steps = %d\", num_train_steps)\n",
    "    \n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "RawResult = collections.namedtuple(\"RawResult\", [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "tf.get_default_graph().finalize()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    if FLAGS.do_train:\n",
    "        train_summary_writer = tf.summary.FileWriter(FLAGS.output_dir + 'summaries/train', sess.graph)\n",
    "        val_summary_writer = tf.summary.FileWriter(FLAGS.output_dir + 'summaries/val')\n",
    "        \n",
    "        f1_list = []\n",
    "        heq_list = []\n",
    "        dheq_list = []\n",
    "        \n",
    "        # Training cycle\n",
    "        for step, batch in enumerate(train_batches):\n",
    "            if step > num_train_steps:\n",
    "                # this means the learning rate has been decayed to 0\n",
    "                break\n",
    "                \n",
    "            batch_features, batch_example_tracker, batch_variation_tracker = batch\n",
    "            \n",
    "\n",
    "            selected_example_features, relative_selected_pos = get_selected_example_features_without_actions(\n",
    "                                                    batch_features, batch_example_tracker, batch_variation_tracker)\n",
    "\n",
    "            fd = convert_features_to_feed_dict(selected_example_features) # feed_dict\n",
    "            try:\n",
    "                _, train_summary, total_loss_res = sess.run([train_op, merged_summary_op, total_loss], \n",
    "                                           feed_dict={unique_ids: fd['unique_ids'], input_ids: fd['input_ids'], \n",
    "                                           input_mask: fd['input_mask'], segment_ids: fd['segment_ids'], \n",
    "                                           start_positions: fd['start_positions'], end_positions: fd['end_positions'], \n",
    "                                           history_answer_marker: fd['history_answer_marker'], training: True})\n",
    "            except:\n",
    "                print('features length: ', len(selected_example_features))\n",
    "\n",
    "            train_summary_writer.add_summary(train_summary, step)\n",
    "            train_summary_writer.flush()\n",
    "            print('training step: {}, total_loss: {}'.format(step, total_loss_res))\n",
    "            \n",
    "            if step >= FLAGS.evaluate_after and step % FLAGS.evaluation_steps == 0 and step != 0:\n",
    "                val_total_loss = []\n",
    "                all_results = []\n",
    "                all_selected_examples = []\n",
    "                all_selected_features = []\n",
    "                \n",
    "                total_num_selected = 0\n",
    "                total_num_actions = 0\n",
    "                total_num_examples = 0\n",
    "                \n",
    "                val_batches = cqa_gen_example_aware_batches(val_features, val_example_tracker, val_variation_tracker, \n",
    "                                           val_example_features_nums, FLAGS.predict_batch_size, 1, shuffle=False)\n",
    "                \n",
    "                for val_batch in val_batches:\n",
    "\n",
    "                    batch_results = []\n",
    "                    batch_features, batch_example_tracker, batch_variation_tracker = val_batch\n",
    "                    \n",
    "                    selected_example_features, relative_selected_pos = get_selected_example_features_without_actions(\n",
    "                                                    batch_features, batch_example_tracker, batch_variation_tracker)\n",
    "\n",
    "                        \n",
    "                    try:\n",
    "                        all_selected_features.extend(selected_example_features)\n",
    "\n",
    "                        fd = convert_features_to_feed_dict(selected_example_features) # feed_dict\n",
    "                        start_logits_res, end_logits_res, batch_total_loss = sess.run([start_logits, end_logits, total_loss], \n",
    "                                    feed_dict={unique_ids: fd['unique_ids'], input_ids: fd['input_ids'], \n",
    "                                    input_mask: fd['input_mask'], segment_ids: fd['segment_ids'], \n",
    "                                    start_positions: fd['start_positions'], end_positions: fd['end_positions'], \n",
    "                                    history_answer_marker: fd['history_answer_marker'], training: False})\n",
    "\n",
    "                        val_total_loss.append(batch_total_loss)\n",
    "\n",
    "                        for each_unique_id, each_start_logits, each_end_logits in zip(fd['unique_ids'], start_logits_res, \n",
    "                                                                                      end_logits_res):  \n",
    "                            each_unique_id = int(each_unique_id)\n",
    "                            each_start_logits = [float(x) for x in each_start_logits.flat]\n",
    "                            each_end_logits = [float(x) for x in each_end_logits.flat]\n",
    "                            batch_results.append(RawResult(unique_id=each_unique_id, start_logits=each_start_logits, \n",
    "                                                           end_logits=each_end_logits))\n",
    "\n",
    "                        all_results.extend(batch_results)\n",
    "                    except:\n",
    "                        print('batch dropped because too large!')\n",
    "\n",
    "                output_prediction_file = os.path.join(FLAGS.output_dir, \"predictions_{}.json\".format(step))\n",
    "                output_nbest_file = os.path.join(FLAGS.output_dir, \"nbest_predictions_{}.json\".format(step))\n",
    "\n",
    "                write_predictions(val_examples, all_selected_features, all_results,\n",
    "                                  FLAGS.n_best_size, FLAGS.max_answer_length,\n",
    "                                  FLAGS.do_lower_case, output_prediction_file,\n",
    "                                  output_nbest_file)\n",
    "\n",
    "                val_total_loss_value = np.average(val_total_loss)\n",
    "                                \n",
    "                \n",
    "                # call the official evaluation script\n",
    "                val_summary = tf.Summary() \n",
    "                val_eval_res = external_call(val_file_json, output_prediction_file)\n",
    "\n",
    "                val_f1 = val_eval_res['f1']\n",
    "                val_followup = val_eval_res['followup']\n",
    "                val_yesno = val_eval_res['yes/no']\n",
    "                val_heq = val_eval_res['HEQ']\n",
    "                val_dheq = val_eval_res['DHEQ']\n",
    "\n",
    "                heq_list.append(val_heq)\n",
    "                dheq_list.append(val_dheq)\n",
    "\n",
    "                val_summary.value.add(tag=\"followup\", simple_value=val_followup)\n",
    "                val_summary.value.add(tag=\"val_yesno\", simple_value=val_yesno)\n",
    "                val_summary.value.add(tag=\"val_heq\", simple_value=val_heq)\n",
    "                val_summary.value.add(tag=\"val_dheq\", simple_value=val_dheq)\n",
    "\n",
    "                print('evaluation: {}, total_loss: {}, f1: {}, followup: {}, yesno: {}, heq: {}, dheq: {}\\n'.format(\n",
    "                    step, val_total_loss_value, val_f1, val_followup, val_yesno, val_heq, val_dheq))\n",
    "                with open(FLAGS.output_dir + 'step_result.txt', 'a') as fout:\n",
    "                        fout.write('{},{},{},{},{},{}\\n'.format(step, val_f1, val_heq, val_dheq, \n",
    "                                            FLAGS.history, FLAGS.output_dir))\n",
    "                \n",
    "                val_summary.value.add(tag=\"total_loss\", simple_value=val_total_loss_value)\n",
    "                val_summary.value.add(tag=\"f1\", simple_value=val_f1)\n",
    "                f1_list.append(val_f1)\n",
    "                \n",
    "                val_summary_writer.add_summary(val_summary, step)\n",
    "                val_summary_writer.flush()\n",
    "                \n",
    "                save_path = saver.save(sess, '{}/model_{}.ckpt'.format(FLAGS.output_dir, step))\n",
    "                print('Model saved in path', save_path)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = max(f1_list)\n",
    "best_f1_idx = f1_list.index(best_f1)\n",
    "best_heq = heq_list[best_f1_idx]\n",
    "best_dheq = dheq_list[best_f1_idx]\n",
    "with open(FLAGS.output_dir + 'result.txt', 'w') as fout:\n",
    "    fout.write('{},{},{},{},{}\\n'.format(best_f1, best_heq, best_dheq, FLAGS.history, FLAGS.output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
